% !TeX root = results.tex
\documentclass[paper.tex]{subfiles}

\begin{document}
\section{Libraries and Data Sources}
To carry out our analysis, we used two datasets. The first was the Delta Cost Project database\footnote{Provided at \url{https://nces.ed.gov/ipeds/deltacostproject/}.}, a dataset derived from the IPEDS datasets containing time series data from the academic years 1986-87 and 2011-12. The second was the College Scorecard dataset,\footnote{Provided at \url{https://collegescorecard.ed.gov/data/}.} which contains data by year from 1996 to 2013 on a variety of measures of student performance, earnings and debt. 

We then recruit help from a handful of freely available Python libraries: 
\begin{itemize}
	\setlength{\itemsep}{-0.3em}
	\item \texttt{numpy}: Standard numerical library, and requirement for any of the below
	\item \texttt{pandas}: Another data manipulation library, that provides handy tools for i/o, manipulation of \texttt{.csv} files in different encodings, and particularly convenient indexing
	\item \texttt{tensorflow}: \cite{tensorflow}, which we use as a basis to construct our recursive neural network 
	\item \texttt{scikit-learn}: \cite{scikit-learn}, a general purpose python machine learning framework that we use to do feature selection, clustering, Principal Component Analysis (PCA), and train our Support Vector Machines.
\end{itemize}

\section{Determining $F$: Training ML Regressors}
As discussed, our goal was to determine the influence of certain financial variables on measures of student success. To this extent, we trained several machine learning classifiers with the financial data from the Delta Cost Project as our features, in an attempt to predict the differentials of student success, calculated from the College Scorecard dataset. We used two regressors: a Support Vector Machine (SVM), and a Recurrent Neural Network (RNN)

\subsection{Data Preprocessing}
The first order of business is to add an additional field to the data set ('adademicyear'), and merge the many files into a single csv file containing information every school and year available. From these large files, we then randomly selected about 10\% of the schools to set aside as blind test sets -- from this point forward, all of the data manipulation is done with the remaining data, and the reported accuracies of sub-components are all generated through cross-validation of this training data. As always, the point of setting aside testing data is to verify that we haven't overfit the model to the noise of the training data -- but even if the machine learning algorithms aren't optimizing on held-out data, its creators inevitably respond to good and bad accuracy scores. So the held-out data is there to protect the algorithm from us. 


\subsection{Support Vector Machines} 
Suppose we have a set $\{ f_i^{(t)} \}_{i = 1}^n$ of financial variables drawn from the Delta Cost Project database, and a set $\{ s_i^{(t)} \}_{i = 1}^m$ of measures of student success. For example, one of these may be the amount of private contributions to a university during the year $t$. Our approach here was to train an SVM regressor for each $s_i^{(t)}$, using the $f_j^{(t)}$ as features. 
\subsubsection{Variable Selection} 
In order to use this work to help decide how to distribute money of the Goodgrant Foundation, we chose as features those variables over which we had direct control. For instance, we chose a variable from the Delta Cost Project database that represented the amount of revenue from private gifts, grants and contracts. We also picked as features the variables that represented the amount of revenue spent on research, student instruction, and so on. A complete list of variables chosen, for both features and targets, can be found in the appendix.  

In addition, to account for the fact that different schools may handle revenue more or less effectively, we used a one-hot encoding of the school ID and added those to our $f_i^{(t)}$. More precisely, if there are $n$ distinct school IDs, then we expand the school ID feature into $n$ single bit features. From now on, we refer to this as the \emph{one-hot encoding} of the feature vectors. 
\subsubsection{Formatting Data for the SVM}
In order to account for the temporality of the data, we used a window of five years when training and evaluating the SVM regressor. If we let $X_u^{(t)}, Y_u^{(t)}$ denote the financial and student success data, respectively, this means that for each school $u$, and for each year $t \in \{1996, ..., 2014\}$, we formed the single example $(h(X_u^{(t)}), Y_u^{(t)} - Y_u^{(t - 1)})$, where \[ h(X_u^{(t)}) := \operatorname{concat}(X_u^{(t - 1)}, ..., X_u^{(t - 5)}), \] and the $\operatorname{concat}$ operator represents the concatenation of these row vectors. 

Due to computational constraints, we were forced to retain only a single copy of the one-hot encoding in each concatenation (as opposed to $5$, the size of the window). While this may also seem like a quite natural decision, this can affect the complexity of the model being learned. While normally the SVM regressor would have had to learn from the data that the same weights are relevant to each of the five one-hot encodings, we have essentially given it free without paying for it by more training examples. 

\subsubsection{Grid Search and Validation}
Following the advice of \cite{chang2011}, we performed a simple grid search over the set of hyperparameters $\{ 0.1, 1, 10, 100, 1000 \} \times \{ 1, 0.01, 0.0001 \}$ where the first set of hyperparameters is the penalty parameter $C$ which penalizes misclassifications, and the second set of hyperparamters are values for $\gamma$, which affect the influence of the label of each datapoint on surrounding datapoints. 


\subsection{Recurrent Neural Networks} Since our data is quite naturally sequential, and recurrent neural networks have had immense success with sequential data, we tried using them for our analysis. In addition, in a recent paper \cite{cho}, an approach is described to do machine translation using Gated Recurrent Units in the form of an encoder and a decoder. The encoder essentially provides a summary of the data, which the decoder attempts to recover.

As described before, our approach consists of several steps, one goal of which is to produce a ranking function. We thought to use these encoder-decoder RNNs since they could essentially learn a single number summary of how the success data was influenced by the financial data. The hope is this is that these single number summaries, perhaps transformed by a softmax, would encode some sort of ranking.  

\subsubsection{Formatting Data for the RNN} Recurrent neural networks naturally work with sequential data. Borrowing the notation from the previous subsection on SVMs, we would like to form the examples $(h(X_u^{(t)}), Y_u^{(t)} - Y_u^{(t - 1)})$ where in this case we have \[ h(X_u^{(t)}) := (X_u^{(t - 1)}, ..., X_u^{(0)}),\] and where we omit the years for which we have no data. Note that one of the advantage of the RNNs reveal themselves here: while filling in missing years with zeros for the SVM is misleading for the SVM, the RNN has no such issue. 


\subsubsection{Architecture}
While our goal was to get an encoder working, in the end our RNN architecture was relatively simple. It consisted of (1) an affine transformation fed into a (2) Gated Recurrent Unit cell, followed by (3) another affine transformation. This choice was a consequence of time constraints and attempting to learn the library over the weekend.

Our preferred architecture would have consisted of three layers of GRU cells: (1) the first layer for input, (2) the second layer consisting of fewer cells for dimensionality compression, and (3) the final layer for output. The memory component of GRU/LSTM cells is improved over the standard RNN cells, and the goal of producing a ranking function are the reasons we chose this architecture.
\end{document}
